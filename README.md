> [!NOTE]
> This README file documents an older project, created during my second year of B.Tech studies. It was generated by Gemini Flash 2.0 (Thinking).

# Music Generation using Performance Events (Jupyter Notebook)

This document provides a detailed explanation of the Jupyter Notebook designed for music generation using a performance-based approach. The notebook leverages the Maestro dataset and preprocessed performance event data to facilitate training music generation models (though the model training itself isn't included in this specific notebook). The primary focus here is on data preparation, augmentation, and the conversion of performance event representations back into audible music (MIDI and WAV formats).

## Overview

This notebook performs the following key tasks:

1.  **Environment Setup:** Installs necessary Python libraries for audio processing, MIDI manipulation, and data handling.
2.  **Data Loading and Preparation:** Defines functions to load the Maestro dataset metadata and create PyTorch datasets for training and validation. These datasets load preprocessed performance event data.
3.  **Data Augmentation:** Implements various data augmentation techniques to expand the training dataset and improve model robustness. These augmentations include random cropping, pitch shifting, and time stretching.
4.  **Performance to Audio Conversion:** Provides utilities to convert the numerical representation of music performance events back into standard MIDI files and then into WAV audio files for listening. This is crucial for evaluating the output of any trained music generation model.

## Dependencies

The notebook relies on the following Python libraries:

* **pyFluidSynth:** Used for converting MIDI files to WAV audio files.
* **music21 (version 6.3.0):** A toolkit for computer-aided musicology, used here for parsing and manipulating MIDI files.
* **torch:** PyTorch, a deep learning framework used for tensor operations and dataset creation.
* **torch.nn:** PyTorch's neural network module.
* **torch.nn.functional:** PyTorch's functional interface.
* **torchaudio:** PyTorch's library for audio and speech processing.
* **torchaudio.transforms:** Transforms for audio data.
* **pandas:** A library for data manipulation and analysis, used for handling the Maestro dataset metadata.
* **matplotlib.pyplot:** Used for basic plotting (e.g., visualizing spectrograms).
* **IPython.display.Audio:** Used for embedding and playing audio within the notebook.
* **os:** For interacting with the operating system (e.g., file path manipulation).
* **random:** For generating random numbers (used in data augmentation).
* **math:** For mathematical operations.
* **typing:** For type hinting.

## Setup

To run this notebook, you will need a Python environment with the above libraries installed. The notebook is designed to run on platforms like Kaggle, which provides a pre-configured environment with many of these libraries.

**Steps:**

1.  **Install pyFluidSynth:** The notebook includes a cell to install `pyFluidSynth` using pip. If you are running this notebook in an environment where system-level packages can be installed (like a local machine with appropriate permissions), you might consider uncommenting the `apt-get install fluidsynth` line for a more robust installation.
2.  **Install music21 (specific version):** The notebook explicitly installs `music21==6.3.0` to ensure compatibility.
3.  **Dataset Location:** The notebook assumes the Maestro dataset (`maestro-v2.0.0`) and a dataset containing preprocessed performance event files (`the-maestro-dataset-performance-events`) are available in the `/kaggle/input/` directory. If you are running this notebook in a different environment, you will need to adjust the file paths accordingly in the `get_df` function.

## Data Loading and Preparation

The notebook defines the following key components for data handling:

* **`get_df(maestro_path, perf_path)`:** This function reads the `maestro-v2.0.0.csv` file, which contains metadata about the Maestro dataset. It adds columns for the paths to the MP3, MIDI, and preprocessed performance event files based on the provided paths. The function also converts categorical columns to the `category` data type for efficiency.
* **`DataframeDataset(df, features, path_col, preload_device)`:** This is a base class for creating PyTorch datasets from pandas DataFrames. It takes a DataFrame, a list of feature columns, a column containing file paths, and an optional device for preloading data into memory (CPU or GPU). It handles the conversion of categorical features to numerical indices. The `_get_data` method is intended to be overridden by subclasses to load the actual data from the specified path.
* **`PerformanceDataset(DataframeDataset)`:** This subclass of `DataframeDataset` is specifically designed for loading the preprocessed performance event files. The `_get_data` method in this class loads the `.pt` files (PyTorch saved tensors) containing the performance events. It also allows for an optional `transform` to be applied to the loaded data.

## Data Augmentation

The notebook implements several data augmentation techniques as PyTorch `nn.Module` subclasses:

* **`Compose(transforms)`:** A simple class to chain multiple transformations together.
* **`RandomCrop1D(size, pad_if_needed, fill)`:** Randomly crops a 1D tensor to a specified `size`. It can optionally pad the tensor if it's smaller than the desired size.
* **`AugmentPitch(half_steps, in_place)`:** Transposes the pitch of the notes in the performance data by a random number of half-steps within the specified range. It operates on note-on and note-off events.
* **`AugmentTime(time_pct, num_steps, in_place)`:** Introduces slight variations in the timing of events by stretching or compressing the time shifts.

These augmentation techniques are applied to the training dataset using the `Compose` class. The validation dataset typically only uses the `RandomCrop1D` transform to ensure consistent input size.

## Performance to Audio Conversion

This section provides the crucial functionality to convert the numerical performance event representation back into audible music:

* **`read_midi(filepath)`:** Reads a MIDI file using the `music21` library.
* **`write_midi(mf, filename)`:** Writes a `music21` MIDI file object to a specified file path.
* **`midi_to_wav(filename)`:** Converts a MIDI file to a WAV audio file using the `fluidsynth` command-line tool. This function assumes `fluidsynth` is installed and accessible in the system's PATH.
* **`play_midi(mf, secs)`:** Takes a `music21` MIDI file object, writes it to a temporary file, converts it to WAV, loads the WAV file using `torchaudio`, and then plays the audio within the notebook using `IPython.display.Audio`.
* **`MidiToPerformanceConverter()`:** This class is responsible for converting between MIDI files (specifically the performance track) and the numerical representation of performance events used in the dataset.
    * **`__init__`:** Initializes mappings between event names (e.g., 'note-on-60', 'time-shift-10') and their corresponding numerical indices.
    * **`midi_to_idxs(self, mf)`:** Converts a `music21` MIDI file object into a list of performance event indices. It extracts tempo information and iterates through the note-on, note-off, time-shift, and velocity events in the performance track.
    * **`make_note(self, track, pitch, velocity)`:** Creates a `music21` MIDI event for a note-on or note-off message.
    * **`idxs_to_midi(self, idxs, verbose)`:** Converts a sequence of performance event indices back into a `music21` MIDI file object. It reconstructs the tempo track and the performance track with the corresponding note-on, note-off, time-shift, and velocity events.

## Usage Example

The notebook demonstrates how to use the conversion functions with a sample data point from the training dataset:

1.  It loads a sample performance event tensor from the `train_dataset`.
2.  It instantiates the `MidiToPerformanceConverter`.
3.  It uses `idxs_to_midi` to convert the tensor back into a `music21` MIDI file object.
4.  It writes the MIDI file to a temporary location using `write_midi`.
5.  It converts the temporary MIDI file to a WAV audio file using `midi_to_wav`.
6.  It loads the WAV file using `torchaudio`.
7.  Finally, it displays the audio using the `display_audio` function, which also shows a Mel spectrogram for visualization.

**Note:** You might encounter a `RuntimeError: Error opening ... : File contains data in an unknown format.` if `fluidsynth` is not correctly installed or if there are issues with the audio file format. Ensure `fluidsynth` is properly installed and that the output path has write permissions. In this specific notebook execution, it seems `fluidsynth` command was not found, indicating a potential installation issue within the Kaggle environment.

## Potential Extensions

This notebook provides a solid foundation for music generation projects. Here are some potential extensions:

* **Implement a Music Generation Model:** Integrate a neural network model (e.g., a Recurrent Neural Network or a Transformer) that can learn to generate sequences of performance events.
* **Training Loop:** Add code to train the implemented model using the prepared `train_dataset` and evaluate its performance on the `val_dataset`.
* **Generation Function:** Create a function that takes a trained model and generates new music by sampling from the model's output.
* **More Sophisticated Data Augmentations:** Explore other data augmentation techniques relevant to music, such as tempo variations or polyphony manipulation.
* **Different Performance Representations:** Investigate alternative ways to represent music performance events.
* **Integration with Other Datasets:** Extend the notebook to work with other music datasets.
