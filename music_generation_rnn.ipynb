{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1246998,"sourceType":"datasetVersion","datasetId":716027},{"sourceId":2887889,"sourceType":"datasetVersion","datasetId":1769046}],"dockerImageVersionId":30146,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# converts midi files to wav files into order to play them\nprint('installing fluidsynth...')\n# !apt-get install fluidsynth --assume-yes --fix-missing > /dev/null\n#pip install pyFluidsynth\n!pip install pyFluidSynth\n\nprint('done!')\n\n# update the version of music21, used to parse midi files\n!pip install music21==6.3.0","metadata":{"execution":{"iopub.status.busy":"2022-12-01T18:18:42.435544Z","iopub.execute_input":"2022-12-01T18:18:42.435876Z","iopub.status.idle":"2022-12-01T18:18:57.515456Z","shell.execute_reply.started":"2022-12-01T18:18:42.435840Z","shell.execute_reply":"2022-12-01T18:18:57.514572Z"},"trusted":true},"outputs":[{"name":"stdout","text":"installing fluidsynth...\nCollecting pyFluidSynth\n  Downloading pyFluidSynth-1.3.1-py3-none-any.whl (19 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pyFluidSynth) (1.19.5)\nInstalling collected packages: pyFluidSynth\nSuccessfully installed pyFluidSynth-1.3.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\ndone!\nRequirement already satisfied: music21==6.3.0 in /opt/conda/lib/python3.7/site-packages (6.3.0)\nRequirement already satisfied: more-itertools in /opt/conda/lib/python3.7/site-packages (from music21==6.3.0) (8.10.0)\nRequirement already satisfied: webcolors in /opt/conda/lib/python3.7/site-packages (from music21==6.3.0) (1.12)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from music21==6.3.0) (1.0.1)\nRequirement already satisfied: chardet in /opt/conda/lib/python3.7/site-packages (from music21==6.3.0) (4.0.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import os, random, math\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nimport torchaudio.transforms as T\nfrom music21 import midi\nfrom IPython.display import Audio\n\nfrom typing import Optional, Tuple\nfrom torch import Tensor","metadata":{"execution":{"iopub.status.busy":"2022-12-01T18:19:29.587022Z","iopub.execute_input":"2022-12-01T18:19:29.587343Z","iopub.status.idle":"2022-12-01T18:19:29.593404Z","shell.execute_reply.started":"2022-12-01T18:19:29.587308Z","shell.execute_reply":"2022-12-01T18:19:29.592209Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Data\n\nCreate a dataframe and pytorch dataset that to load the performance data for training. This combines the Maestro dataset with another dataset that contains the preprocessed performance files.","metadata":{}},{"cell_type":"code","source":"def get_df(\n    maestro_path = '/kaggle/input/themaestrodatasetv2/maestro-v2.0.0/',\n    perf_path = '/kaggle/input/the-maestro-dataset-performance-events/maestro-v2.0.0/',\n):\n    df = pd.read_csv(f'{maestro_path}/maestro-v2.0.0.csv')\n    for col in ['canonical_composer', 'canonical_title', 'split', 'year']:\n        df[col] = df[col].astype('category')\n    df['mp3_path'] = maestro_path + df.audio_filename.str.split('.').str.get(0) + '.mp3'\n    df['midi_path'] = maestro_path + df.midi_filename\n    df['perf_path'] = perf_path + df.midi_filename + '.pt'\n    return df\n\n\nget_df().head()","metadata":{"execution":{"iopub.status.busy":"2022-12-01T18:19:31.330612Z","iopub.execute_input":"2022-12-01T18:19:31.330884Z","iopub.status.idle":"2022-12-01T18:19:31.399058Z","shell.execute_reply.started":"2022-12-01T18:19:31.330855Z","shell.execute_reply":"2022-12-01T18:19:31.398411Z"},"trusted":true},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"   canonical_composer                canonical_title       split  year  \\\n0          Alban Berg                   Sonata Op. 1       train  2018   \n1          Alban Berg                   Sonata Op. 1       train  2008   \n2          Alban Berg                   Sonata Op. 1       train  2017   \n3  Alexander Scriabin  24 Preludes Op. 11, No. 13-24       train  2004   \n4  Alexander Scriabin               3 Etudes, Op. 65  validation  2006   \n\n                                       midi_filename  \\\n0  2018/MIDI-Unprocessed_Chamber3_MID--AUDIO_10_R...   \n1  2008/MIDI-Unprocessed_03_R2_2008_01-03_ORIG_MI...   \n2  2017/MIDI-Unprocessed_066_PIANO066_MID--AUDIO-...   \n3  2004/MIDI-Unprocessed_XP_21_R1_2004_01_ORIG_MI...   \n4  2006/MIDI-Unprocessed_17_R1_2006_01-06_ORIG_MI...   \n\n                                      audio_filename    duration  \\\n0  2018/MIDI-Unprocessed_Chamber3_MID--AUDIO_10_R...  698.661160   \n1  2008/MIDI-Unprocessed_03_R2_2008_01-03_ORIG_MI...  759.518471   \n2  2017/MIDI-Unprocessed_066_PIANO066_MID--AUDIO-...  464.649433   \n3  2004/MIDI-Unprocessed_XP_21_R1_2004_01_ORIG_MI...  872.640588   \n4  2006/MIDI-Unprocessed_17_R1_2006_01-06_ORIG_MI...  397.857508   \n\n                                            mp3_path  \\\n0  /kaggle/input/themaestrodatasetv2/maestro-v2.0...   \n1  /kaggle/input/themaestrodatasetv2/maestro-v2.0...   \n2  /kaggle/input/themaestrodatasetv2/maestro-v2.0...   \n3  /kaggle/input/themaestrodatasetv2/maestro-v2.0...   \n4  /kaggle/input/themaestrodatasetv2/maestro-v2.0...   \n\n                                           midi_path  \\\n0  /kaggle/input/themaestrodatasetv2/maestro-v2.0...   \n1  /kaggle/input/themaestrodatasetv2/maestro-v2.0...   \n2  /kaggle/input/themaestrodatasetv2/maestro-v2.0...   \n3  /kaggle/input/themaestrodatasetv2/maestro-v2.0...   \n4  /kaggle/input/themaestrodatasetv2/maestro-v2.0...   \n\n                                           perf_path  \n0  /kaggle/input/the-maestro-dataset-performance-...  \n1  /kaggle/input/the-maestro-dataset-performance-...  \n2  /kaggle/input/the-maestro-dataset-performance-...  \n3  /kaggle/input/the-maestro-dataset-performance-...  \n4  /kaggle/input/the-maestro-dataset-performance-...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>canonical_composer</th>\n      <th>canonical_title</th>\n      <th>split</th>\n      <th>year</th>\n      <th>midi_filename</th>\n      <th>audio_filename</th>\n      <th>duration</th>\n      <th>mp3_path</th>\n      <th>midi_path</th>\n      <th>perf_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Alban Berg</td>\n      <td>Sonata Op. 1</td>\n      <td>train</td>\n      <td>2018</td>\n      <td>2018/MIDI-Unprocessed_Chamber3_MID--AUDIO_10_R...</td>\n      <td>2018/MIDI-Unprocessed_Chamber3_MID--AUDIO_10_R...</td>\n      <td>698.661160</td>\n      <td>/kaggle/input/themaestrodatasetv2/maestro-v2.0...</td>\n      <td>/kaggle/input/themaestrodatasetv2/maestro-v2.0...</td>\n      <td>/kaggle/input/the-maestro-dataset-performance-...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Alban Berg</td>\n      <td>Sonata Op. 1</td>\n      <td>train</td>\n      <td>2008</td>\n      <td>2008/MIDI-Unprocessed_03_R2_2008_01-03_ORIG_MI...</td>\n      <td>2008/MIDI-Unprocessed_03_R2_2008_01-03_ORIG_MI...</td>\n      <td>759.518471</td>\n      <td>/kaggle/input/themaestrodatasetv2/maestro-v2.0...</td>\n      <td>/kaggle/input/themaestrodatasetv2/maestro-v2.0...</td>\n      <td>/kaggle/input/the-maestro-dataset-performance-...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Alban Berg</td>\n      <td>Sonata Op. 1</td>\n      <td>train</td>\n      <td>2017</td>\n      <td>2017/MIDI-Unprocessed_066_PIANO066_MID--AUDIO-...</td>\n      <td>2017/MIDI-Unprocessed_066_PIANO066_MID--AUDIO-...</td>\n      <td>464.649433</td>\n      <td>/kaggle/input/themaestrodatasetv2/maestro-v2.0...</td>\n      <td>/kaggle/input/themaestrodatasetv2/maestro-v2.0...</td>\n      <td>/kaggle/input/the-maestro-dataset-performance-...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Alexander Scriabin</td>\n      <td>24 Preludes Op. 11, No. 13-24</td>\n      <td>train</td>\n      <td>2004</td>\n      <td>2004/MIDI-Unprocessed_XP_21_R1_2004_01_ORIG_MI...</td>\n      <td>2004/MIDI-Unprocessed_XP_21_R1_2004_01_ORIG_MI...</td>\n      <td>872.640588</td>\n      <td>/kaggle/input/themaestrodatasetv2/maestro-v2.0...</td>\n      <td>/kaggle/input/themaestrodatasetv2/maestro-v2.0...</td>\n      <td>/kaggle/input/the-maestro-dataset-performance-...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Alexander Scriabin</td>\n      <td>3 Etudes, Op. 65</td>\n      <td>validation</td>\n      <td>2006</td>\n      <td>2006/MIDI-Unprocessed_17_R1_2006_01-06_ORIG_MI...</td>\n      <td>2006/MIDI-Unprocessed_17_R1_2006_01-06_ORIG_MI...</td>\n      <td>397.857508</td>\n      <td>/kaggle/input/themaestrodatasetv2/maestro-v2.0...</td>\n      <td>/kaggle/input/themaestrodatasetv2/maestro-v2.0...</td>\n      <td>/kaggle/input/the-maestro-dataset-performance-...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"class DataframeDataset(torch.utils.data.Dataset):\n    # df is a dataframe that has a path column (path_col) for each file\n    # features is a list of columns in the dataframe that will be added to items\n    # preload_device can be cpu/cuda\n    def __init__(self, df, features=[], path_col='path', preload_device=None):\n        self.df = df.copy()\n        for feature in features:\n            if self.df[feature].dtype.name == 'category':\n                # convert categories to indices\n                self.df[feature] = self.df[feature].cat.codes\n\n        self.features = features\n        self.path_col = path_col\n        self.preloaded_data = None\n        if preload_device is not None:\n            self.preloaded_features = []\n            self.preloaded_data = []\n            for i, row in self.df.iterrows():\n                self.preloaded_features.append({\n                    k: torch.tensor(v).to(preload_device)\n                    for k, v in row[features].to_dict().items()\n                })\n                self.preloaded_data.append(\n                    self._get_data(row[self.path_col]).to(preload_device)\n                )\n\n    def __len__(self):\n        return(len(self.df))\n\n    def __getitem__(self, index):\n        if self.preloaded_data is None: \n            row = self.df.iloc[index]\n            features = row[self.features].to_dict()\n            data = self._get_data(row[self.path_col])\n        else:\n            features = self.preloaded_features[index]\n            data = self.preloaded_data[index]\n        return {**features, 'data': data}\n\n    def _get_data(self, path):\n        raise NotImplementedError()\n\n\nclass PerformanceDataset(DataframeDataset):\n    def __init__(self, df, features=[], path_col='perf_path', preload_device=None, transform=None):\n        self.transform = transform\n        super().__init__(df, features, path_col, preload_device)\n\n    def _get_data(self, path):\n        return torch.jit.load(path).state_dict()['0'].long()\n\n    def __getitem__(self, index):\n        item = super().__getitem__(index)\n        if self.transform is not None:\n            item['data'] = self.transform(item['data'])\n        return item","metadata":{"execution":{"iopub.status.busy":"2022-12-01T18:19:33.107125Z","iopub.execute_input":"2022-12-01T18:19:33.107932Z","iopub.status.idle":"2022-12-01T18:19:33.121727Z","shell.execute_reply.started":"2022-12-01T18:19:33.107893Z","shell.execute_reply":"2022-12-01T18:19:33.120962Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Data augmentation\n\nWe can expand the dataset by adding some augmentations, including random cropping, increasing/decreasing pitch, and tweaking time delays.","metadata":{}},{"cell_type":"code","source":"class Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n    def __call__(self, x):\n        for transform in self.transforms:\n            x = transform(x)\n        return x\n    \n    \nclass RandomCrop1D:\n    def __init__(self, size, pad_if_needed=True, fill=0):\n        self.size = size\n        self.pad_if_needed = pad_if_needed\n        self.fill = fill\n    def __call__(self, x):\n        x_len = x.shape[-1]\n        len_diff = x_len - self.size\n        pad_needed = len_diff < 0\n        if pad_needed and not self.pad_if_needed:\n            raise Exception('too small')\n        start = random.randint(0, max(0, len_diff))\n        crop = x[..., start:start+self.size]\n        if pad_needed:\n            return F.pad(crop, pad=(0, max(0, -len_diff)), value=self.fill)\n        return crop\n    \n\nclass AugmentPitch:\n    def __init__(self, half_steps=3, in_place=True):\n        self.half_steps = half_steps\n        self.in_place = in_place\n\n    def __call__(self, x):\n        if not self.in_place:\n            x = x.clone()\n        pitch_transposition = random.randint(-self.half_steps, self.half_steps)\n        note_ons = (x >= 0) * (x < 128)\n        x[note_ons] = torch.clamp(x[note_ons] + pitch_transposition, 0, 127)\n        note_offs = (x >= 128) * (x < 256)\n        x[note_offs] = torch.clamp(x[note_offs] + pitch_transposition, 128, 255)\n        return x\n\n\nclass AugmentTime:\n    def __init__(self, time_pct=2.5, num_steps=5, in_place=True):\n        self.time_pct = time_pct / 100\n        self.in_place = in_place\n        self.num_steps = num_steps\n\n    def __call__(self, x):\n        if not self.in_place:\n            x = x.clone()\n        time_stretch = (1 - self.time_pct * 2) + random.randint(0, self.num_steps - 1) * self.time_pct\n        time_shifts = (x >= 256) * (x < 356)\n        x[time_shifts] = torch.clamp((x[time_shifts] - 256) * time_stretch, 0, 99).long() + 256\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-12-01T18:19:35.138055Z","iopub.execute_input":"2022-12-01T18:19:35.138363Z","iopub.status.idle":"2022-12-01T18:19:35.151656Z","shell.execute_reply.started":"2022-12-01T18:19:35.138331Z","shell.execute_reply":"2022-12-01T18:19:35.150507Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"df = get_df()\n\ntrain_dataset = PerformanceDataset(\n    df[df.split == 'train'],\n    transform=Compose([\n        RandomCrop1D(2048, fill=388),\n        AugmentPitch(in_place=False),\n        AugmentTime(),\n    ]),\n    preload_device='cuda',\n)\n\nval_dataset = PerformanceDataset(\n    df[df.split == 'validation'],\n    transform=Compose([\n        RandomCrop1D(2048, fill=388),\n    ]),\n    preload_device='cuda',\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T18:19:37.322625Z","iopub.execute_input":"2022-12-01T18:19:37.322899Z","iopub.status.idle":"2022-12-01T18:19:39.728224Z","shell.execute_reply.started":"2022-12-01T18:19:37.322867Z","shell.execute_reply":"2022-12-01T18:19:39.727460Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## Convert data into MIDI and WAV files\n\nVisualize and listen to performance tensors by converting them in MIDI/WAV. This code will be used to convert the output of a model that is trained to output the perforamnce representation into audible music.","metadata":{}},{"cell_type":"code","source":"def read_midi(filepath):\n  mf = midi.MidiFile()\n  mf.open(filepath)\n  mf.read()\n  mf.close()\n  return mf\n\n\ndef write_midi(mf, filename = 'tempmidi.mid'):\n  mf.open(filename, attrib='wb')\n  mf.write()\n  mf.close()\n  return filename\n\n\ndef midi_to_wav(filename):\n  # linux ships with a default midi font\n  !fluidsynth -F $filename\\.wav -r 16000 -i -n -T wav /usr/share/sounds/sf2/FluidR3_GM.sf2 $filename > /dev/null\n  return filename + '.wav'\n\n\ndef play_midi(mf, secs):\n  data, rate = torchaudio.load(midi_to_wav(write_midi(mf)))\n  display(Audio(data[0, :16000 * secs], rate=rate))\n\n    \nclass MidiToPerformanceConverter:\n    def __init__(self):\n        self.event_to_idx = {}\n        for i in range(128):\n          self.event_to_idx['note-on-' + str(i)] = i\n        for i in range(128):\n          self.event_to_idx['note-off-' + str(i)] = i + 128\n        for i in range(100):\n          self.event_to_idx['time-shift-' + str(i + 1)] = i + 128 + 128\n        for i in range(32):\n          self.event_to_idx['velocity-' + str(i)] = i + 128 + 128 + 100\n        self.idx_to_event = list(self.event_to_idx.keys())\n        self.num_channels = len(self.idx_to_event)\n\n        \n    def midi_to_idxs(self, mf):\n      event_to_idx = self.event_to_idx\n      ticks_per_beat = mf.ticksPerQuarterNote\n      # The maestro dataset uses the first track to store tempo data\n      tempo_data = next(e for e in mf.tracks[0].events if e.type == midi.MetaEvents.SET_TEMPO).data\n      # tempo data is stored at microseconds per beat (beat = quarter note)\n      microsecs_per_beat = int.from_bytes(tempo_data, 'big')\n      millis_per_tick = microsecs_per_beat / ticks_per_beat / 1e3\n\n      idxs = []\n      started = False\n      previous_t = None\n      is_pedal_down = False\n      notes_to_turn_off = set()\n      notes_on = set()\n\n      # The second track stores the actual performance\n      for e in mf.tracks[1].events:\n        #if started and e.type == 'DeltaTime' and e.time > 0:\n        if e.type == 'DeltaTime' and e.time > 0:\n          # event times are stored as ticks, so convert to milliseconds\n          millis = e.time * millis_per_tick\n\n          # combine repeated delta time events\n          t = millis + (0 if previous_t is None else previous_t)\n\n          # we can only represent a max time of 1 second (1000 ms)\n          # so we must split up times that are larger than that into separate events\n          while t > 0:\n            t_chunk = min(t, 1000)\n            idx = event_to_idx['time-shift-' + str(math.ceil(t_chunk / 10))]\n            if previous_t is None:\n              idxs.append(idx)\n            else:\n              idxs[-1] = idx\n              previous_t = None\n            t -= t_chunk\n          previous_t = t_chunk\n\n        elif e.type == midi.ChannelVoiceMessages.NOTE_ON:\n          if e.velocity == 0:\n            if is_pedal_down:\n              notes_to_turn_off.add(e.pitch)\n            elif e.pitch in notes_on:\n              idxs.append(event_to_idx['note-off-' + str(e.pitch)])\n              notes_on.remove(e.pitch)\n              previous_t = None\n          else:\n            if e.pitch in notes_to_turn_off:\n              idxs.append(event_to_idx['note-off-' + str(e.pitch)])\n              notes_to_turn_off.remove(e.pitch)\n              notes_on.remove(e.pitch)\n\n            # midi supports 128 velocities, but our representation only allows 32\n            idxs.append(event_to_idx['velocity-' + str(e.velocity // 4)])\n            idxs.append(event_to_idx['note-on-' + str(e.pitch)])\n            notes_on.add(e.pitch)\n            started = True\n            previous_t = None\n\n        elif e.type == midi.ChannelVoiceMessages.CONTROLLER_CHANGE and e.parameter1 == 64: # sustain pedal\n          # pedal values greater than 64 mean the pedal is being held down,\n          # otherwise it's up\n          if is_pedal_down and e.parameter2 < 64:\n            is_pedal_down = False\n            for pitch in notes_to_turn_off:\n              idxs.append(event_to_idx['note-off-' + str(pitch)])\n              notes_on.remove(pitch)\n            notes_to_turn_off = set()\n            previous_t = None\n          elif not is_pedal_down and e.parameter2 >= 64:\n            is_pedal_down = True\n            previous_t = None\n\n      return idxs\n\n\n    def make_note(self, track, pitch, velocity):\n      e = midi.MidiEvent(track, type=midi.ChannelVoiceMessages.NOTE_ON, channel=1)\n      e.pitch = int(pitch)\n      e.velocity = int(velocity)\n      return e\n\n\n    def idxs_to_midi(self, idxs, verbose=False):\n      if type(idxs) == torch.Tensor:\n        idxs = idxs.detach().numpy()\n\n      mf = midi.MidiFile()\n      mf.ticksPerQuarterNote = 1024\n\n      # The maestro dataset uses the first track to store tempo data, and the second\n      # track to store the actual performance. So follow that convention.\n      tempo_track = midi.MidiTrack(0)\n      track = midi.MidiTrack(1)\n      mf.tracks = [tempo_track, track]\n\n      tempo = midi.MidiEvent(tempo_track, type=midi.MetaEvents.SET_TEMPO)\n      # temp.data is the number of microseconds per beat (per quarter note)\n      # So to set ticks per millis = 1 (easy translation from time-shift values to ticks),\n      # tempo.data must be 1e3 * 1024, since ticksPerQuarterNote is 1024 (see above)\n      tempo.data = int(1e3 * 1024).to_bytes(3, 'big')\n\n      end_of_track = midi.MidiEvent(tempo_track, type=midi.MetaEvents.END_OF_TRACK)\n      end_of_track.data = ''\n      tempo_track.events = [\n        # there must always be a delta time before each event\n        midi.DeltaTime(tempo_track, time=0),\n        tempo,\n        midi.DeltaTime(tempo_track, time=0),\n        end_of_track\n      ]\n\n      track.events = [midi.DeltaTime(track, time=0)]\n\n      # set to 0 initially in case no velocity events occur before the first note\n      current_velocity = 0\n      notes_on = set()\n      errors = {'is_on': 0, 'is_not_on': 0}\n\n      for idx in idxs:\n        if 0 <= idx < 128: # note-on\n          pitch = idx\n          if pitch in notes_on:\n            if verbose:\n              print(pitch, 'is already on')\n            errors['is_on'] += 1\n            continue\n          if track.events[-1].type != 'DeltaTime':\n            track.events.append(midi.DeltaTime(track, time=0))\n          track.events.append(self.make_note(track, pitch, current_velocity))\n          notes_on.add(pitch)\n\n        elif 128 <= idx < 256: # note-off\n          pitch = idx - 128\n          if pitch not in notes_on:\n            if verbose:\n              print(pitch, 'is not on')\n            errors['is_not_on'] += 1\n            continue\n          if track.events[-1].type != 'DeltaTime':\n            track.events.append(midi.DeltaTime(track, time=0))\n          track.events.append(self.make_note(track, pitch, 1))\n          notes_on.remove(pitch)\n\n        elif 256 <= idx < 356: # time-shift\n          t = (1 + idx - 256) * 10\n          if track.events[-1].type == 'DeltaTime':\n            # combine repeated delta times\n            track.events[-1].time += t\n          else:\n            track.events.append(midi.DeltaTime(track, time=t))\n\n        else: # velocity\n          current_velocity = (idx - 356) * 4\n\n      if verbose:\n        print('remaining notes left on:', notes_on)\n\n      if track.events[-1].type != 'DeltaTime':\n        track.events.append(midi.DeltaTime(track, time=0))\n      track.events.append(end_of_track)\n\n      return mf, errors","metadata":{"execution":{"iopub.status.busy":"2022-12-01T18:19:42.189125Z","iopub.execute_input":"2022-12-01T18:19:42.189444Z","iopub.status.idle":"2022-12-01T18:19:42.238013Z","shell.execute_reply.started":"2022-12-01T18:19:42.189410Z","shell.execute_reply":"2022-12-01T18:19:42.237225Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def display_audio(title, audio):\n    plt.title(title)\n    plt.imshow(torch.log10(T.MelSpectrogram(n_mels=80)(audio) + 1e-6)[0])\n    plt.show()\n    display(Audio(audio, rate=16000))","metadata":{"execution":{"iopub.status.busy":"2022-12-01T18:19:47.729887Z","iopub.execute_input":"2022-12-01T18:19:47.730182Z","iopub.status.idle":"2022-12-01T18:19:47.734890Z","shell.execute_reply.started":"2022-12-01T18:19:47.730125Z","shell.execute_reply":"2022-12-01T18:19:47.733946Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"idxs = train_dataset[0]['data'].to('cpu')\n\nconverter = MidiToPerformanceConverter()\nmf = converter.idxs_to_midi(idxs)[0]\nmidi_filename = write_midi(mf)\nwav_filename = midi_to_wav(midi_filename)\nmidi_to_wav_audio = torchaudio.load(wav_filename)\nmidi_to_wav_audio = midi_to_wav_audio[0][:1, :16000*10]\n\ndisplay_audio('performance events to midi to wav', midi_to_wav_audio)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T18:19:49.399125Z","iopub.execute_input":"2022-12-01T18:19:49.399738Z","iopub.status.idle":"2022-12-01T18:19:50.428686Z","shell.execute_reply.started":"2022-12-01T18:19:49.399699Z","shell.execute_reply":"2022-12-01T18:19:50.426840Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/bin/bash: fluidsynth: command not found\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_19/3927362868.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmidi_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite_midi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwav_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmidi_to_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidi_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmidi_to_wav_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmidi_to_wav_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmidi_to_wav_audio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchaudio/backend/soundfile_backend.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \"\"\"\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msoundfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"WAV\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    627\u001b[0m         self._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    628\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;31m# Move write position to 0 (like in Python file objects)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid file: {0!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         _error_check(_snd.sf_error(file_ptr),\n\u001b[0;32m-> 1184\u001b[0;31m                      \"Error opening {0!r}: \".format(self.name))\n\u001b[0m\u001b[1;32m   1185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode_int\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSFM_WRITE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m             \u001b[0;31m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/soundfile.py\u001b[0m in \u001b[0;36m_error_check\u001b[0;34m(err, prefix)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_error_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error opening 'tempmidi.mid.wav': System error."],"ename":"RuntimeError","evalue":"Error opening 'tempmidi.mid.wav': System error.","output_type":"error"}],"execution_count":26},{"cell_type":"markdown","source":"# Model\n\nCreate the PerformanceRNN model, which embedds performance events into an internal embedding, learns sequences autoregressively, and then projects its internal representation back to a distribution over possible events. \n\nThe model has a forward method to predict the next event given a sequence, as well as a forward_seq method that efficiently predicts N subsequent elements into the future by feeding its own predicted back into itself in a loop.","metadata":{}},{"cell_type":"code","source":"class PerformanceRNN(nn.Module):\n    def __init__(self, num_embedding, hidden_dim=128, num_layers=3):\n        super(PerformanceRNN, self).__init__()\n        self.input_embedding = nn.Embedding(num_embeddings=num_embedding, embedding_dim=hidden_dim)\n        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n        self.output_linear = nn.Linear(in_features=hidden_dim, out_features=num_embedding)\n\n    # x is (batch, seq)\n    # returns (batch, seq, num_embedding)\n    def forward(\n        self,\n        x: torch.Tensor,\n        state: Optional[Tuple[torch.Tensor, torch.Tensor]]=None,\n    ) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:\n        x = self.input_embedding(x)\n        x, state = self.lstm(x, state) # (batch, seq, hidden)\n        x = self.output_linear(x)\n        return x, state\n\n    # prime is (batch, prime_seq)\n    # returns (batch, prime_seq + steps)\n    def forward_seq(self, prime, steps, greedy):\n        if greedy:\n            sample = lambda x: torch.argmax(x, axis=2)\n        else:\n            sample = lambda x: torch.distributions.Categorical(torch.softmax(x, axis=2)).sample()\n\n        x, state = self.forward(prime)\n        output = sample(x)\n        for _ in range(steps - 1):\n            x, state = self.forward(output[:, -1:], state)\n            x = sample(x)\n            output = torch.cat([output, x], axis=1)\n\n        return torch.cat([prime, output], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T07:57:53.816094Z","iopub.execute_input":"2021-12-10T07:57:53.816369Z","iopub.status.idle":"2021-12-10T07:57:53.838161Z","shell.execute_reply.started":"2021-12-10T07:57:53.816336Z","shell.execute_reply":"2021-12-10T07:57:53.837347Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_batch(batch):\n    data = batch['data']\n    return data[..., :-1], data[..., 1:]\n\n\ncel = torch.nn.CrossEntropyLoss()\ndef lossfn(prediction, y):\n    return cel(prediction.transpose(1, 2), y)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T07:57:53.839525Z","iopub.execute_input":"2021-12-10T07:57:53.840241Z","iopub.status.idle":"2021-12-10T07:57:53.921987Z","shell.execute_reply.started":"2021-12-10T07:57:53.840197Z","shell.execute_reply":"2021-12-10T07:57:53.92114Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DEVICE = 'cuda'\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16)\nmodel = PerformanceRNN(converter.num_channels + 1, hidden_dim=512).to(DEVICE)\n\nx, y = prepare_batch(next(iter(train_dataloader)))\nx = x.to(DEVICE)\ny = y.to(DEVICE)\nprediction = model(x)[0]\nloss = lossfn(prediction, y)\n\ny_seq = model.forward_seq(x[:, :1], 10, greedy=False)\ny_seq_greedy = model.forward_seq(x[:, :1], 10, greedy=True)\nx.shape, y.shape, prediction.shape, loss.shape, y_seq.shape, y_seq_greedy.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-10T07:57:53.923459Z","iopub.execute_input":"2021-12-10T07:57:53.92372Z","iopub.status.idle":"2021-12-10T07:57:55.891022Z","shell.execute_reply.started":"2021-12-10T07:57:53.923686Z","shell.execute_reply":"2021-12-10T07:57:55.890342Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train\n\nTrain the model using the performance data loaded above, recording train/validation performance.","metadata":{}},{"cell_type":"code","source":"def train(\n    model,\n    optimizer,\n    lossfn,\n    train_dataloader,\n    val_dataloader,\n    train_cb,\n    val_cb,\n    prepare_batch,\n    num_epochs,\n):\n    for epoch in range(num_epochs):\n        print(f'## EPOCH {epoch} - train - {len(train_dataloader)} batches ##')\n        model.train()\n        for i, batch in enumerate(train_dataloader):\n            x, y = prepare_batch(batch)\n            prediction = model(x)[0]\n            loss = lossfn(prediction, y)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            train_cb(i, prediction, y, loss)\n\n        print(f'## EPOCH {epoch} - validation - {len(val_dataloader)} batches ##')\n        model.eval()\n        with torch.no_grad():\n            for i, batch in enumerate(val_dataloader):\n                x, y = prepare_batch(batch)\n                prediction = model(x)[0]\n                loss = lossfn(prediction, y)\n                val_cb(i, prediction, y, loss)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T07:57:55.892475Z","iopub.execute_input":"2021-12-10T07:57:55.892722Z","iopub.status.idle":"2021-12-10T07:57:55.901238Z","shell.execute_reply.started":"2021-12-10T07:57:55.892689Z","shell.execute_reply":"2021-12-10T07:57:55.899841Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_cb(dataloader):\n    losses = []\n    acc = []\n    num_batches = len(dataloader)\n    def cb(i, prediction, y, loss):\n        losses.append(loss)\n        acc.append((torch.argmax(prediction, -1) == y).float().mean())\n        #print(i, end=' ' if i < num_batches - 1 else '\\n')\n    return losses, acc, cb\n\n\n# linearly interpolate b to match a's length, for displaying val/train loss\ndef interpolate(a, b):\n    return F.interpolate(\n        torch.tensor(b)[None, None],\n        len(a),\n        mode='linear',\n        align_corners=True,\n    )[0, 0]","metadata":{"execution":{"iopub.status.busy":"2021-12-10T07:57:55.902861Z","iopub.execute_input":"2021-12-10T07:57:55.903129Z","iopub.status.idle":"2021-12-10T07:57:55.91434Z","shell.execute_reply.started":"2021-12-10T07:57:55.903094Z","shell.execute_reply":"2021-12-10T07:57:55.913621Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = PerformanceRNN(converter.num_channels + 1, hidden_dim=512).to('cuda')\noptimizer = torch.optim.Adam(model.parameters())\n\ntrain_losses, train_acc, train_cb = make_cb(train_dataloader)\nval_losses, val_acc, val_cb = make_cb(val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T07:57:55.915638Z","iopub.execute_input":"2021-12-10T07:57:55.916278Z","iopub.status.idle":"2021-12-10T07:57:55.974675Z","shell.execute_reply.started":"2021-12-10T07:57:55.916245Z","shell.execute_reply":"2021-12-10T07:57:55.974023Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train(\n    model,\n    optimizer,\n    lossfn,\n    train_dataloader,\n    val_dataloader,\n    train_cb,\n    val_cb,\n    prepare_batch,\n    num_epochs=10,\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T07:57:55.97574Z","iopub.execute_input":"2021-12-10T07:57:55.976051Z","iopub.status.idle":"2021-12-10T09:42:31.801098Z","shell.execute_reply.started":"2021-12-10T07:57:55.976014Z","shell.execute_reply":"2021-12-10T09:42:31.80037Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 4)) \nplt.subplot(1, 2, 1)\nplt.title('loss')\nplt.plot(train_losses, label='train')\nplt.plot(interpolate(train_losses, val_losses), label='validation')\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.title('accuracy')\nplt.ylim(0, 1)\nplt.plot(train_acc, label='train')\nplt.plot(interpolate(train_acc, val_acc), label='validation')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:42:31.802948Z","iopub.execute_input":"2021-12-10T09:42:31.803296Z","iopub.status.idle":"2021-12-10T09:42:32.714255Z","shell.execute_reply.started":"2021-12-10T09:42:31.803252Z","shell.execute_reply":"2021-12-10T09:42:32.713595Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference\n\nNow that we've trained a model, use it to generate music! One way to do this is to pass some events from an existing piece into the model and ask it to predict subsequent events. We can then compare the ground truth to the generated version.","metadata":{}},{"cell_type":"code","source":"def convert_and_display(title, idxs):\n    mf = converter.idxs_to_midi(idxs.to('cpu'))[0]\n    midi_filename = write_midi(mf)\n    wav_filename = midi_to_wav(midi_filename)\n    midi_to_wav_audio = torchaudio.load(wav_filename)\n    midi_to_wav_audio = midi_to_wav_audio[0][:1]\n    display_audio(title, midi_to_wav_audio)\n    plt.title('events')\n    plt.plot(idxs.squeeze().to('cpu'))\n    plt.show()\n\n\n# first 256 events is the first ~20 secs\nidxs = train_dataset.preloaded_data[0][:256]\n\n# ground truth\nconvert_and_display('ground truth', idxs)\n\n# greedy prediction\nprime = idxs[:64] # first 64 events is the first ~5 secs\nprime = prime[None] # add batch\nprediction = model.forward_seq(prime, steps=256 - 64, greedy=True)\nconvert_and_display('greedy prediction', prediction[0])\n\n# sampling prediction\nprediction = model.forward_seq(prime, steps=256 - 64, greedy=False)\nconvert_and_display('sample prediction', prediction[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:42:32.715331Z","iopub.execute_input":"2021-12-10T09:42:32.71571Z","iopub.status.idle":"2021-12-10T09:42:39.038028Z","shell.execute_reply.started":"2021-12-10T09:42:32.715677Z","shell.execute_reply":"2021-12-10T09:42:39.037287Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Export model","metadata":{}},{"cell_type":"code","source":"model.eval()\nmodel.to('cpu')\ntorch.jit.script(model).save('performance_rnn.pt')\nmodel_loaded = torch.jit.load('performance_rnn.pt')\nmodel_loaded(torch.zeros(1, 1).long())[0].shape","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:47:30.447826Z","iopub.execute_input":"2021-12-10T09:47:30.448116Z","iopub.status.idle":"2021-12-10T09:47:30.602721Z","shell.execute_reply.started":"2021-12-10T09:47:30.448086Z","shell.execute_reply":"2021-12-10T09:47:30.601977Z"},"trusted":true},"outputs":[],"execution_count":null}]}